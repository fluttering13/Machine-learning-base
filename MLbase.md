# Machine-learning
>哈利，你居然用我的魔法來對付我的魔法

來說說一些 從數據科學角度出發去看ML 去看這些oracle 不能僅僅只是煉丹爐或者魔法

## 機器學習(What is machine learning?)
在切入主題之前，我們來定義一下什麼是學習？

是把所有問題的答案都背下來？是個學習嗎？

很明顯，從小到大如果你有朋友是這樣，也許它會過得很辛苦。

上述的過程也只能說是一個資料庫，什麼是學習呢，在於使用這些資料的能力。

你會看到這些資料是怎麼彼此建立起他們的連結，

例如說我現在資料裡面有 大象、猴子、企鵝

我們試著去建立起他們的屬性，透過觀察，我們可以知道這些都是哺乳類動物，他們都在動物園裡面

這些資料跟資料之間可以透過函數的連結，幫助我們去找到他們之間的關係

如果重建這些關係，我們就說是學習的一個過程

但往往我們可能不太清楚現實世界的關係是長什麼樣子，所以我們會有一個hypothesis set

例如說我們把動物這樣的概念當成一個hypothesis set

數據上你今天知道了 大象、猴子、企鵝

所以你試圖去刻劃動物這樣的集合，

`所以你是去看他們共有的器官`

`他們有 眼睛、鼻子、嘴巴......等等。`

`得知現有的數據 (大象、猴子、企鵝)`

`在有目標函數的情況 (共有的器官)`

`試圖去找到一個最佳化的參數 (眼睛、鼻子、嘴巴)`

`來描述你有興趣的模型 (動物)`


所以很自然的，我們會問說，要怎麼去建立起一個模式去找到這樣的函數關係？
$$f(X) = Y$$

>Laplace 準確定性概念(Laplace's quasi-deterministic conception)
>>所有的隨機模型(古典)，都可以用函數模型來進行表示

在現今常用的方式我們試圖去寫下一個類線性的方程
也就是所謂的類神經元網路
$$\alpha W{\rm{X + b = Y}}$$

$$
\alpha 
\left[
\begin{matrix}
    w_{11} & ... & w_{1n} \\\\
    ... & ... & ... \\\\
    w_{m1} & ... & w_{mn}
\end{matrix}
\right]
\left[
\begin{matrix}
x_{1}\\\\
...\\\\
x_{m}
\end{matrix}
\right]
+
\left[
\begin{matrix}
b_{1}\\\\
...\\\\
b_{m}
\end{matrix}
\right]
\leq
\left[
\begin{matrix}
y_{1}\\\\
...\\\\
y_{m}
\end{matrix}
\right]
$$

其中 $\alpha$ 可以是一個非線性的方程
在這邊我們只是寫下他們的形式，其中會遇到各式各樣的問題
如：
優化怎麼做？
目標方程要怎麼選？
要選什麼樣的 $\alpha$ ?

再來我們從一些數據科學的角度來進行切入

# 機器學習能不能學
從上述的雛形，我們可以知道我們是想要找到一個方程或是特定的機率分布來了解我們想知道的hypothesis

以下我們漸漸去放入一些根據統計假設，逐漸得出我們需要多少的數據

## 有限數據 (Finite statistics)
往往我們是想要追求理想上的模型

但我們今天只能夠從真實數據拿到的是有限的數量

這邊會衍生一些問題

我要拿多少的數據才能能夠很好的去回答一個問題？

此處我們來引入一個Hoeffding不等式來說明

假設我們從一個母體作抽樣，抽樣過程都是獨立同分布(i.i.d) 總共抽N筆資料，

把採樣集的結果，所佔的比例為 $\mu$，

把測試集的結果，所佔的比例為 $\nu$

我們令這兩的比例的誤差為 $\epsilon$

$$P[{\rm{|}}\mu {\rm{ - }}\nu {\rm{| < }}\varepsilon ] \ge 2\exp \left( { - 2{\varepsilon ^2}N} \right)$$

假如今天取了1000筆的資料，我們允許這個誤差可以到7%

我們會得到機率為0.00011090319886435389 約為0.01%

我們取了10000筆，這個機率可以降到10^-43

**只要我們採樣(sample)了足夠多的數量N，一定是更有信心去做預測的**

上述是我們只對一個hypothesis做思考，但假如我們今天有M個hypothesis呢？從union bound可以給出
$$P[{\rm{|}}\mu {\rm{ - }}\nu {\rm{| < }}\varepsilon ] \ge 2 M \exp \left( { - 2{\varepsilon ^2}N} \right)$$
這是因為我們今天對M個hypothesis之間沒有做限制，而這會導致這個bound在N很大的時候會發散掉，

例如說在PLA裡面，M就可以有無窮多線去拆分不同多個標籤

**小的 $M$，可以讓我們做到 取樣集錯誤率 $\mu$ 跟 測試集錯誤率 $\nu$ 很接近，但太少的選擇 $M$ 不一定找到很小的錯誤率**

**大的 $M$，看起來做不到取樣集錯誤率 $\mu$ 跟 測試集錯誤率 $\nu$ 很接近，於是我們要引入限制，讓 $P$ 還是可以收斂**

那這個限制從那裡來呢？




